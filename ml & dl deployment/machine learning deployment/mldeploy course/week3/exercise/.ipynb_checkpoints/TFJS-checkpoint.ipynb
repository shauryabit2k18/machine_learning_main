{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Using Tesnsorflow Version:  1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print('\\u2022 Using Tesnsorflow Version: ' , tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring The Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/cats_and_dogs_filtered.zip: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate \\\n",
    "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "  -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xzipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8d8c32090080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlocalzip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/tmp/cats_and_dogs_filtered.zip'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mzip_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_zip\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/tmp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xzipfile' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "localzip = '/tmp/cats_and_dogs_filtered.zip'\n",
    "\n",
    "zip_ref = xzipfile.ZipFile(local_zip , 'r')\n",
    "\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "\n",
    "train_dir = os.path.join(base_dir , 'train')\n",
    "validation_dir = os.path.join(base_dir , 'validation')\n",
    "\n",
    "# directory with our training cat/dog pictures\n",
    "train_cats_dir = os.path.join(train_dir , 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir , 'dogs')\n",
    "\n",
    "# directory with our validation cat/dog pictures\n",
    "validation_cats_dir = os.path.join(validation_dir , 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir , 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "\n",
    "print(train_cat_fnames[:10])\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images :' , len(os.listdir(  train_cats_dir)))\n",
    "print('total training dog images :' , len(os.listdir(   train_dogs_dir)))\n",
    "\n",
    "print('total validation cat images :' , len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images :' , len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parametes of our graph \n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "pic_index = 0 #index for iterating over images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup matplotlib fig , and size it to fir=t 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols*4 , nrows*4)\n",
    "\n",
    "pic_index +=8\n",
    "\n",
    "next_cat_pix = [os.path.join(train_cats_dir , fname)\n",
    "               for fname in train_cat_fname[pic_index-8:pic_index]]\n",
    "\n",
    "next_dog_pix = [os.path.join(train_dogs_dir , fname)\n",
    "               for fname in train_dog_fname[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix + next_dog_pix):\n",
    "    # set up subplot; subplot indices start at 1\n",
    "    sp = plt.subplot(nrows , ncols , i+1)\n",
    "    sp.axis('Off')#dont show axes (or gridlines)\n",
    "    \n",
    "    img = mpimg.imread(img_path)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Small Model from Scratch to Get to ~72% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    tf.keras.layers.Conv2D(16 , (3,3) , activation = 'relu' , input_shape = (150 , 150 , 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2 , 2),\n",
    "    tf.keras.layers.Conv2D(32 , (3 , 3) , activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2 , 2),\n",
    "    tf.keras.layers.Conv2D(64 , (3 , 3) , activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2 , 2),\n",
    "    # Flatten to feed into  a DNN\n",
    "    tf.keras.layers.Flatten()\n",
    "    # 512 Neuron hidden layer\n",
    "    tf.keras.layers.Dense(512 , activation = \"relu\"),\n",
    "    # only one output neuron as it is a binary classification\n",
    "    tf.keras.layers.Dense(1 , activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.optimizers import RMSprop\n",
    "\n",
    "comdel.compile(optimizer = RMSprop(lr = 0.001) , loss = 'binary_crossentropy' , metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# all images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "test_datagen = ImageDataGenerator(rescale 1.0/255.)\n",
    "\n",
    "# flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(train_dir , batch_size = 20 , class_mode = 'binary' , target_size = (150 , 150))\n",
    "\n",
    "# flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir , batch_size = 20 , class_mode = 'binary' , target_size = (150 , 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator , validation_data = validation_generator , \n",
    "                             steps_per_epoch = 100 , epochs = 15 , validation_steps = 50 ,\n",
    "                             verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy and loss for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs , acc , label = 'Training')\n",
    "plt.plot(epochs , val_acc , label = 'Validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot(epochs , loss , label = 'Training')\n",
    "plt.plot(epochs , val_loss , lable = 'Vlidation')\n",
    "plt.legend()\n",
    "plt.title('Training amd validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"./my_model.h5\"\n",
    "\n",
    "model.save(saved.predict([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Tensorflow.js Convertor on The Saved Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflowjs_convertor -- input_format = keras {saved_model_path} ./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
